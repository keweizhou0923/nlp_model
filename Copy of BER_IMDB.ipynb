{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of BER_IMDB.ipynb","version":"0.3.2","provenance":[{"file_id":"1D_o5yVGb92sPD8UZlN1R84458F8PpwIW","timestamp":1556641387732}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"rMcnRgdP9uS7","colab_type":"text"},"cell_type":"markdown","source":["## This file aim to implement BERT to predict the rating for the IMBD data.\n"]},{"metadata":{"id":"8HF2IjjiDGR2","colab_type":"text"},"cell_type":"markdown","source":["A lot of the tasks that we did overlapped with ELMo (NER and MultiNLI in particular). For ULMFit unfortunately there was no overlap between BERT/ELMo/OpenAI GPT vs. ULMFit. The other thing that ULMFit explores that we didn't is in-domain LM adaptation. But we suspect that BERT will do quite well on task like IMDB, especially with LM adaptation (i.e., running pre-trianing on the unsupervised IMDB documents for 10,000 steps starting from the BERT model, before fine-tuning)."]},{"metadata":{"id":"CsHXKmseAb8a","colab_type":"code","colab":{}},"cell_type":"code","source":["folder = 'multi_cased_L-12_H-768_A-12'\n","download_url = 'https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip'  # ссылка на скачивание модели\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GW4hEIbuAe3a","colab_type":"code","outputId":"0e6ceead-fa8e-43da-8a3a-c7f3c3059613","executionInfo":{"status":"ok","timestamp":1556641464397,"user_tz":300,"elapsed":28368,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"15007109988165336330"}},"colab":{"base_uri":"https://localhost:8080/","height":1349}},"cell_type":"code","source":["print('Downloading model...')\n","zip_path = '{}.zip'.format(folder)\n","!test -d $folder || (wget $download_url && unzip $zip_path)\n","\n","# скачиваем из BERT репозитория файл tokenization.py\n","!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n","\n","# install Keras BERT\n","!pip install keras-bert\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading model...\n","--2019-04-30 16:23:56--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.202.128, 2607:f8b0:4001:c03::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.202.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 662903077 (632M) [application/zip]\n","Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n","\n","multi_cased_L-12_H- 100%[===================>] 632.19M   140MB/s    in 4.6s    \n","\n","2019-04-30 16:24:01 (137 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n","\n","Archive:  multi_cased_L-12_H-768_A-12.zip\n","   creating: multi_cased_L-12_H-768_A-12/\n","  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n","  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n","  inflating: multi_cased_L-12_H-768_A-12/vocab.txt  \n","  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.index  \n","  inflating: multi_cased_L-12_H-768_A-12/bert_config.json  \n","--2019-04-30 16:24:10--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12257 (12K) [text/plain]\n","Saving to: ‘tokenization.py’\n","\n","tokenization.py     100%[===================>]  11.97K  --.-KB/s    in 0s      \n","\n","2019-04-30 16:24:10 (134 MB/s) - ‘tokenization.py’ saved [12257/12257]\n","\n","Collecting keras-bert\n","  Downloading https://files.pythonhosted.org/packages/37/d2/f1d7d632c717fa994b267d1efda6d83b8318de0980c20f2d149be465e88e/keras-bert-0.41.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.16.3)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.2.4)\n","Collecting keras-pos-embd==0.9.0 (from keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/56/5e/7b1e933104a25f2039b6788e392a650671e3bcbee6404ea29dcb92295614/keras-pos-embd-0.9.0.tar.gz\n","Collecting keras-transformer==0.21.0 (from keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/d3/3a/ad25f5c71adc6b8aa73f71b1367be873b4103125a614ba57c006d1a9b1ff/keras-transformer-0.21.0.tar.gz\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.9)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.8.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.7)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.2.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n","Collecting keras-multi-head==0.18.0 (from keras-transformer==0.21.0->keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/d7/5d/8156def9ca75c55bb87819618e9a3e1f8e587c722570e2e93ad616b9269d/keras-multi-head-0.18.0.tar.gz\n","Collecting keras-layer-normalization==0.11.0 (from keras-transformer==0.21.0->keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/c6/b0/c786d5a5e79d985281a06da0a1f3f559cf425921464e6b07b9f1cb093a5a/keras-layer-normalization-0.11.0.tar.gz\n","Collecting keras-position-wise-feed-forward==0.4.0 (from keras-transformer==0.21.0->keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/91/21/4eefba0b6ea01de9c6e469970a39dbdbce14e5183a20274d9a181f55eaa8/keras-position-wise-feed-forward-0.4.0.tar.gz\n","Collecting keras-embed-sim==0.3.0 (from keras-transformer==0.21.0->keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/8e/16/b05954f9578ded225fd1bd56154ade949782c03b668a1fc424d5050e868a/keras-embed-sim-0.3.0.tar.gz\n","Collecting keras-self-attention==0.39.0 (from keras-multi-head==0.18.0->keras-transformer==0.21.0->keras-bert)\n","  Downloading https://files.pythonhosted.org/packages/91/70/51150779d5bbd1488a30c62026b141073873faf81eac7a62c6460cb5efe0/keras-self-attention-0.39.0.tar.gz\n","Building wheels for collected packages: keras-bert, keras-pos-embd, keras-transformer, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n","  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/3c/b7/23/f6d90d13439fbb4bb428863b9d8654d97859fcd2e2c7ef9f13\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/fb/97/65/170068ed0a4bd2185d561afee6c93e23e87e8d735d61389590\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/19/9f/ff/3b38f44f6db035cfd33cff4909edcc4864a6aeec80d9deaf23\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/9f/88/83/d7680876b48974c3c11fc334ed1d0a480ae218764062385bf3\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/86/dc/2e/3ac54a6b948bff68cb999d210c6ebf9e22df7a4a24cf114436\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/40/a1/13/3c913efde102d56ac584f61004a9fec6f8859b6feec6aa7aa7\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/bf/f2/c6/0610efe9730c708b24ec29c25cebd38eb485acbc2eee7b5634\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/6c/d6/3e/cac34bf035198e38947006910f3ecb25613d6d9d76ea6d8ef2\n","Successfully built keras-bert keras-pos-embd keras-transformer keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n","Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n","Successfully installed keras-bert-0.41.0 keras-embed-sim-0.3.0 keras-layer-normalization-0.11.0 keras-multi-head-0.18.0 keras-pos-embd-0.9.0 keras-position-wise-feed-forward-0.4.0 keras-self-attention-0.39.0 keras-transformer-0.21.0\n"],"name":"stdout"}]},{"metadata":{"id":"wRExgb4K9qdU","colab_type":"code","outputId":"740acbf9-9078-4f25-dd9a-446fc7643d9b","executionInfo":{"status":"ok","timestamp":1556641467509,"user_tz":300,"elapsed":27495,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"15007109988165336330"}},"colab":{"base_uri":"https://localhost:8080/","height":113}},"cell_type":"code","source":["import tensorflow as tf\n","import pandas as pd\n","import tensorflow_hub as hub\n","import os\n","import keras_bert\n","import re\n","import numpy as np\n","from keras_bert import load_trained_model_from_checkpoint\n","import tokenization\n","from tokenization import FullTokenizer\n","from tqdm import tqdm_notebook\n","from tensorflow.keras import backend as K\n","\n","# Initialize session\n","sess = tf.Session()\n","\n","# Params for bert model and tokenization\n","bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","max_seq_length = 256"],"execution_count":3,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0430 16:24:25.216726 139713547454336 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n","Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"rh57ll9G-Zn9","colab_type":"code","outputId":"7a5faf0b-0301-41ba-dc71-f2fe2f3e9366","executionInfo":{"status":"ok","timestamp":1556641501702,"user_tz":300,"elapsed":58932,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"15007109988165336330"}},"colab":{"base_uri":"https://localhost:8080/","height":240}},"cell_type":"code","source":["# Load all files from a directory in a DataFrame.\n","def load_directory_data(directory):\n","  data = {}\n","  data[\"sentence\"] = []\n","  data[\"sentiment\"] = []\n","  for file_path in os.listdir(directory):\n","    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n","      data[\"sentence\"].append(f.read())\n","      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n","  return pd.DataFrame.from_dict(data)\n","\n","# Merge positive and negative examples, add a polarity column and shuffle.\n","def load_dataset(directory):\n","  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n","  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n","  pos_df[\"polarity\"] = 1\n","  neg_df[\"polarity\"] = 0\n","  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n","\n","# Download and process the dataset files.\n","def download_and_load_datasets(force_download=False):\n","  dataset = tf.keras.utils.get_file(\n","      fname=\"aclImdb.tar.gz\", \n","      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n","      extract=True)\n","\n","  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                       \"aclImdb\", \"train\"))\n","  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                      \"aclImdb\", \"test\"))\n","\n","  return train_df, test_df\n","\n","# Reduce logging output.\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","train_df, test_df = download_and_load_datasets()\n","train_df.head()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","84131840/84125825 [==============================] - 3s 0us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>sentiment</th>\n","      <th>polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>This whirling movie looks more like a combinat...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>This one and \"Her Pilgrim Soul\" are two of my ...</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I first saw this when I was around 7. I rememb...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Otto Preminger directs this light as a feather...</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I've come to realise from watching Euro horror...</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence sentiment  polarity\n","0  This whirling movie looks more like a combinat...         1         0\n","1  This one and \"Her Pilgrim Soul\" are two of my ...         8         1\n","2  I first saw this when I was around 7. I rememb...         3         0\n","3  Otto Preminger directs this light as a feather...         4         0\n","4  I've come to realise from watching Euro horror...         2         0"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"-LJ5aMaFBmEL","colab_type":"code","outputId":"e6764b6e-b993-45a3-b852-f70df29d8f22","executionInfo":{"status":"ok","timestamp":1556641501703,"user_tz":300,"elapsed":55318,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"15007109988165336330"}},"colab":{"base_uri":"https://localhost:8080/","height":297}},"cell_type":"code","source":["train_df.describe()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>25000.00000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.50000</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.50001</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.50000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1.00000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.00000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          polarity\n","count  25000.00000\n","mean       0.50000\n","std        0.50001\n","min        0.00000\n","25%        0.00000\n","50%        0.50000\n","75%        1.00000\n","max        1.00000"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"UfOolKp3B5Ss","colab_type":"code","outputId":"6ad36472-b862-4e11-fdf1-d898db212649","executionInfo":{"status":"ok","timestamp":1556641502014,"user_tz":300,"elapsed":53795,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"15007109988165336330"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"cell_type":"code","source":["train_df.groupby(\"polarity\")[\"sentiment\"].value_counts()\n"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["polarity  sentiment\n","0         1            5100\n","          4            2696\n","          3            2420\n","          2            2284\n","1         10           4732\n","          8            3009\n","          7            2496\n","          9            2263\n","Name: sentiment, dtype: int64"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"2tJDvqHnX0Fp","colab_type":"code","outputId":"e687f29b-c4cc-4780-da8e-808f3dac92bb","executionInfo":{"status":"ok","timestamp":1556641502015,"user_tz":300,"elapsed":51934,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"15007109988165336330"}},"colab":{"base_uri":"https://localhost:8080/","height":161}},"cell_type":"code","source":["train_df.info()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 25000 entries, 0 to 24999\n","Data columns (total 3 columns):\n","sentence     25000 non-null object\n","sentiment    25000 non-null object\n","polarity     25000 non-null int64\n","dtypes: int64(1), object(2)\n","memory usage: 586.0+ KB\n"],"name":"stdout"}]},{"metadata":{"id":"B-oJylxhC0Wh","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","# Create datasets (Only take up to max_seq_length words for memory)\n","train_text = train_df['sentence'].tolist()\n","train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n","train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n","train_label = train_df['polarity'].tolist()\n","\n","test_text = test_df['sentence'].tolist()\n","test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n","test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n","test_label = test_df['polarity'].tolist()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5Fd919BeC_mf","colab_type":"code","outputId":"02b8bfa2-128d-4148-f9d2-b7f789de6296","executionInfo":{"status":"ok","timestamp":1556641708504,"user_tz":300,"elapsed":254149,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"15007109988165336330"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":["class PaddingInputExample(object):\n","    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n","  When running eval/predict on the TPU, we need to pad the number of examples\n","  to be a multiple of the batch size, because the TPU requires a fixed batch\n","  size. The alternative is to drop the last batch, which is bad because it means\n","  the entire output data won't be generated.\n","  We use this class instead of `None` because treating `None` as padding\n","  battches could cause silent errors.\n","  \"\"\"\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","    def __init__(self, guid, text_a, text_b=None, label=None):\n","        \"\"\"Constructs a InputExample.\n","    Args:\n","      guid: Unique id for the example.\n","      text_a: string. The untokenized text of the first sequence. For single\n","        sequence tasks, only this sequence must be specified.\n","      text_b: (Optional) string. The untokenized text of the second sequence.\n","        Only must be specified for sequence pair tasks.\n","      label: (Optional) string. The label of the example. This should be\n","        specified for train and dev examples, but not for test examples.\n","    \"\"\"\n","        self.guid = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.label = label\n","\n","def create_tokenizer_from_hub_module():\n","    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","    bert_module =  hub.Module(bert_path)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    vocab_file, do_lower_case = sess.run(\n","        [\n","            tokenization_info[\"vocab_file\"],\n","            tokenization_info[\"do_lower_case\"],\n","        ]\n","    )\n","\n","    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","def convert_single_example(tokenizer, example, max_seq_length=256):\n","    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n","\n","    if isinstance(example, PaddingInputExample):\n","        input_ids = [0] * max_seq_length\n","        input_mask = [0] * max_seq_length\n","        segment_ids = [0] * max_seq_length\n","        label = 0\n","        return input_ids, input_mask, segment_ids, label\n","\n","    tokens_a = tokenizer.tokenize(example.text_a)\n","    if len(tokens_a) > max_seq_length - 2:\n","        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n","\n","    tokens = []\n","    segment_ids = []\n","    tokens.append(\"[CLS]\")\n","    segment_ids.append(0)\n","    for token in tokens_a:\n","        tokens.append(token)\n","        segment_ids.append(0)\n","    tokens.append(\"[SEP]\")\n","    segment_ids.append(0)\n","\n","    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","    # tokens are attended to.\n","    input_mask = [1] * len(input_ids)\n","\n","    # Zero-pad up to the sequence length.\n","    while len(input_ids) < max_seq_length:\n","        input_ids.append(0)\n","        input_mask.append(0)\n","        segment_ids.append(0)\n","\n","    assert len(input_ids) == max_seq_length\n","    assert len(input_mask) == max_seq_length\n","    assert len(segment_ids) == max_seq_length\n","\n","    return input_ids, input_mask, segment_ids, example.label\n","\n","def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n","    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n","\n","    input_ids, input_masks, segment_ids, labels = [], [], [], []\n","    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n","        input_id, input_mask, segment_id, label = convert_single_example(\n","            tokenizer, example, max_seq_length\n","        )\n","        input_ids.append(input_id)\n","        input_masks.append(input_mask)\n","        segment_ids.append(segment_id)\n","        labels.append(label)\n","    return (\n","        np.array(input_ids),\n","        np.array(input_masks),\n","        np.array(segment_ids),\n","        np.array(labels).reshape(-1, 1),\n","    )\n","\n","def convert_text_to_examples(texts, labels):\n","    \"\"\"Create InputExamples\"\"\"\n","    InputExamples = []\n","    for text, label in zip(texts, labels):\n","        InputExamples.append(\n","            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n","        )\n","    return InputExamples\n","\n","# Instantiate tokenizer\n","tokenizer = create_tokenizer_from_hub_module()\n","\n","# Convert data to InputExample format\n","train_examples = convert_text_to_examples(train_text, train_label)\n","test_examples = convert_text_to_examples(test_text, test_label)\n","\n","# Convert to features\n","(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n","(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28f7b1b16c024ad582e7c2a366e6f1ad","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Converting examples to features', max=25000, style=ProgressSt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64dfa94d6fe349fa8d78957b6fc7f513","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Converting examples to features', max=25000, style=ProgressSt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"metadata":{"id":"-4ejDuNGEhP7","colab_type":"code","colab":{}},"cell_type":"code","source":["class BertLayer(tf.layers.Layer):\n","    def __init__(self, n_fine_tune_layers=10, **kwargs):\n","        self.n_fine_tune_layers = n_fine_tune_layers\n","        self.trainable = True\n","        self.output_size = 768\n","        super(BertLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.bert = hub.Module(\n","            bert_path,\n","            trainable=self.trainable,\n","            name=\"{}_module\".format(self.name)\n","        )\n","\n","        trainable_vars = self.bert.variables\n","\n","        # Remove unused layers\n","        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n","\n","        # Select how many layers to fine tune\n","        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n","\n","        # Add to trainable weights\n","        for var in trainable_vars:\n","            self._trainable_weights.append(var)\n","            \n","        for var in self.bert.variables:\n","            if var not in self._trainable_weights:\n","                self._non_trainable_weights.append(var)\n","\n","        super(BertLayer, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n","        input_ids, input_mask, segment_ids = inputs\n","        bert_inputs = dict(\n","            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n","        )\n","        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n","            \"pooled_output\"\n","        ]\n","        return result\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], self.output_size)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d0ImjR0zElHQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# Build model\n","def build_model(max_seq_length): \n","    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n","    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n","    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n","    bert_inputs = [in_id, in_mask, in_segment]\n","    \n","    bert_output = BertLayer(n_fine_tune_layers=3)(bert_inputs)\n","    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n","    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n","    \n","    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    model.summary()\n","    \n","    return model\n","\n","def initialize_vars(sess):\n","    sess.run(tf.local_variables_initializer())\n","    sess.run(tf.global_variables_initializer())\n","    sess.run(tf.tables_initializer())\n","    K.set_session(sess)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f0bP6Yg5Em46","colab_type":"code","outputId":"0919f613-04ec-480d-f177-78aa938c9efe","executionInfo":{"status":"ok","timestamp":1556643870643,"user_tz":300,"elapsed":596267,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"15007109988165336330"}},"colab":{"base_uri":"https://localhost:8080/","height":505}},"cell_type":"code","source":["\n","model = build_model(max_seq_length)\n","\n","# Instantiate variables\n","initialize_vars(sess)\n","\n","\n","model.fit(\n","    [train_input_ids, train_input_masks, train_segment_ids], \n","    train_labels,\n","    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n","    epochs=2,\n","    batch_size=32\n",")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_ids (InputLayer)          (None, 256)          0                                            \n","__________________________________________________________________________________________________\n","input_masks (InputLayer)        (None, 256)          0                                            \n","__________________________________________________________________________________________________\n","segment_ids (InputLayer)        (None, 256)          0                                            \n","__________________________________________________________________________________________________\n","bert_layer_1 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n","                                                                 input_masks[0][0]                \n","                                                                 segment_ids[0][0]                \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 256)          196864      bert_layer_1[0][0]               \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n","==================================================================================================\n","Total params: 110,302,011\n","Trainable params: 3,147,009\n","Non-trainable params: 107,155,002\n","__________________________________________________________________________________________________\n","Train on 25000 samples, validate on 25000 samples\n","Epoch 1/2\n","25000/25000 [==============================] - 1078s 43ms/sample - loss: 0.3230 - acc: 0.8602 - val_loss: 0.2843 - val_acc: 0.8778\n","Epoch 2/2\n","25000/25000 [==============================] - 1075s 43ms/sample - loss: 0.2367 - acc: 0.9064 - val_loss: 0.2560 - val_acc: 0.8906\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f110317e668>"]},"metadata":{"tags":[]},"execution_count":12}]},{"metadata":{"id":"BH1VIG6KEoNw","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","preds = model.predict([test_input_ids[:], \n","                                test_input_masks[:], \n","                                test_segment_ids[:]]\n","                              ) # predictions before we clear and reload model\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1fKKoDVdEwD1","colab_type":"code","outputId":"77e85a05-039e-4204-da2b-ea46791febdc","executionInfo":{"status":"ok","timestamp":1553657063277,"user_tz":300,"elapsed":1176,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"08043202712657867536"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["np.shape(pre_save_preds)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(100, 1)"]},"metadata":{"tags":[]},"execution_count":34}]},{"metadata":{"id":"vZQ60P_DLQLs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":135},"outputId":"813f9b07-3357-4c7f-d65b-cbb3f57d8ae6","executionInfo":{"status":"error","timestamp":1556644010935,"user_tz":300,"elapsed":114,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"15007109988165336330"}}},"cell_type":"code","source":["preds = model.predict([test_input_ids[0:2]\n","                                test_input_masks[0:2], \n","                                test_segment_ids[0:2]])"],"execution_count":16,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-bae75adc1095>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    test_input_masks[0:2],\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"metadata":{"id":"qQqoDmH9NU_v","colab_type":"code","colab":{}},"cell_type":"code","source":["pred = [p[0] for p in  preds.tolist()]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LNtw3kF94bA3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":179},"outputId":"7693118d-2834-4216-d561-cb8458bcc960","executionInfo":{"status":"ok","timestamp":1556644741136,"user_tz":300,"elapsed":648,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"15007109988165336330"}}},"cell_type":"code","source":["tmp = [ p>0.5 for p in  pred]\n","from sklearn.metrics import classification_report\n","print(classification_report(test_df[\"polarity\"] ,tmp))\n","\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.85      0.95      0.90     12500\n","           1       0.94      0.83      0.88     12500\n","\n","   micro avg       0.89      0.89      0.89     25000\n","   macro avg       0.90      0.89      0.89     25000\n","weighted avg       0.90      0.89      0.89     25000\n","\n"],"name":"stdout"}]},{"metadata":{"id":"G4T_60PNNBab","colab_type":"code","outputId":"48c61cb5-8e40-42ae-9fb1-b5f8ae964528","executionInfo":{"status":"ok","timestamp":1556644741138,"user_tz":300,"elapsed":4,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"15007109988165336330"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["np.mean(test_df[\"polarity\"]== tmp)\n","# epoch 1 accu"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8906"]},"metadata":{"tags":[]},"execution_count":25}]},{"metadata":{"id":"oi7lpGytPFEK","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"D0VLbvjEL4WQ","colab_type":"code","outputId":"9ca1c6ec-9fd7-4cf5-bac3-728d55cb756d","executionInfo":{"status":"ok","timestamp":1553657983193,"user_tz":300,"elapsed":603,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"08043202712657867536"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["np.mean(test_df[\"polarity\"][:1000] == tmp)\n","# epoch 1 accu"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.878"]},"metadata":{"tags":[]},"execution_count":65}]},{"metadata":{"id":"BnTrE3C3On5S","colab_type":"code","outputId":"491ad23a-1f53-4734-9f95-0cdde56f6f41","executionInfo":{"status":"ok","timestamp":1553657956375,"user_tz":300,"elapsed":1193,"user":{"displayName":"Kewei Zhou","photoUrl":"","userId":"08043202712657867536"}},"colab":{"base_uri":"https://localhost:8080/","height":1071}},"cell_type":"code","source":["test_df[\"sentiment\"][:1000] "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       4\n","1      10\n","2      10\n","3       8\n","4       3\n","5       7\n","6       7\n","7       7\n","8       8\n","9       9\n","10      1\n","11      4\n","12     10\n","13      9\n","14      9\n","15      7\n","16      1\n","17     10\n","18      4\n","19     10\n","20      2\n","21      2\n","22      8\n","23      2\n","24      7\n","25      1\n","26      8\n","27      1\n","28      2\n","29      9\n","       ..\n","970    10\n","971     1\n","972     1\n","973     2\n","974     1\n","975     1\n","976     8\n","977     9\n","978     9\n","979     3\n","980     1\n","981     4\n","982     8\n","983    10\n","984     2\n","985    10\n","986     7\n","987     7\n","988     2\n","989    10\n","990     4\n","991     4\n","992     2\n","993     1\n","994    10\n","995     2\n","996     1\n","997     1\n","998     1\n","999     7\n","Name: sentiment, Length: 1000, dtype: object"]},"metadata":{"tags":[]},"execution_count":61}]},{"metadata":{"id":"-7JA9-puOwMz","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}